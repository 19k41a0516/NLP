{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/19k41a0516/NLP/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzsVTkHd52Tb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O68Vb96f9l3t"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/training_set_rel3.tsv\", sep='\\t', encoding='ISO-8859-1');\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvCqMxuh92As"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soq-1oju95Ua"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC1sE0YB9_jY"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmri-C28-GFO"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeqJhOmP-PXY"
      },
      "outputs": [],
      "source": [
        "df[df['essay_set']==7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DC0PSb9-XTp"
      },
      "outputs": [],
      "source": [
        "df.dropna(axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maADRKbZ-lkc"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHWSNqjp-nuS"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z91lNIoK-wtv"
      },
      "outputs": [],
      "source": [
        "df['essay_set'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1MiDBrr-2k8"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['rater1_domain1','rater2_domain1'],inplace=True,axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qzgsap2_TyS"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5HJVf7b_Ydv"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJuxplK6_aYZ"
      },
      "outputs": [],
      "source": [
        "min(df['domain1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMhSCDeY_gAX"
      },
      "outputs": [],
      "source": [
        "max(df['domain1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7qeF_dV_n3q"
      },
      "outputs": [],
      "source": [
        "df.skew()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09CIabgT_sth"
      },
      "outputs": [],
      "source": [
        "df.kurtosis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucTewNO0_1Ds"
      },
      "outputs": [],
      "source": [
        "#Check Skewness\n",
        "sns.distplot(df['domain1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f0pV5RdABhN"
      },
      "outputs": [],
      "source": [
        "df[df['essay_set']==2]['domain1_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_KgCJ55AOdA"
      },
      "outputs": [],
      "source": [
        "min_range = [2,1,0,0,0,0,0,0]\n",
        "max_range = [12,6,3,3,4,4,30,60]\n",
        "\n",
        "def normalize(x,mi,ma):\n",
        "    #print(\"Before Normalization: \"+str(x))\n",
        "    x = (x-mi)/(ma-mi)\n",
        "    #print(\"After Normalization : \"+str(x))\n",
        "    return round(x*10)\n",
        "\n",
        "df['final_score']=df.apply(lambda x:normalize(x['domain1_score'],min_range[x['essay_set']-1],max_range[x['essay_set']-1]),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh148of5Aecc"
      },
      "outputs": [],
      "source": [
        "df['final_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQhTi4RPAgqe"
      },
      "outputs": [],
      "source": [
        "sns.distplot(df['final_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JHX6wnVA0T_"
      },
      "outputs": [],
      "source": [
        "df.skew()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfienS-lA3FU"
      },
      "outputs": [],
      "source": [
        "df.skew()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3aOsfyLA7uT"
      },
      "outputs": [],
      "source": [
        "df.skew()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoGxxSVqA__U"
      },
      "outputs": [],
      "source": [
        "df.skew()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIJmTl8KBEgt"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQnivqOCBSEj"
      },
      "source": [
        "PRE_PROCESSING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIrNhdzGBcNS"
      },
      "outputs": [],
      "source": [
        "def clean_essay(essay):\n",
        "    x=[]\n",
        "    for i in essay.split():\n",
        "        if i.startswith(\"@\"):\n",
        "            continue\n",
        "        else:\n",
        "            x.append(i)\n",
        "    return ' '.join(x)\n",
        "\n",
        "df['essay'] = df['essay'].apply(lambda x:clean_essay(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61J0Xm_pDRf-"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL2B3WofDk1T"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvfuRJrqELj-"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "def remove_stop_words(essay):\n",
        "    word_tokens = word_tokenize(essay) \n",
        "    filtered_sentence = [] \n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "df['clean_essay'] = df['essay'].apply(lambda x:remove_stop_words(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvKL2eooEbG3"
      },
      "outputs": [],
      "source": [
        "def remove_puncs(essay):\n",
        "    essay = re.sub(\"[^A-Za-z ]\",\"\",essay)\n",
        "    return essay\n",
        "\n",
        "df['clean_essay'] = df['clean_essay'].apply(lambda x:remove_puncs(x))·πÅ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAk10K1VEg1r"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGUpCVSNEjAd"
      },
      "outputs": [],
      "source": [
        "def sent2word(x):\n",
        "    x=re.sub(\"[^A-Za-z0-9]\",\" \",x)\n",
        "    words=nltk.word_tokenize(x)\n",
        "    return words\n",
        "\n",
        "def essay2word(essay):\n",
        "    essay = essay.strip()\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw = tokenizer.tokenize(essay)\n",
        "    final_words=[]\n",
        "    for i in raw:\n",
        "        if(len(i)>0):\n",
        "            final_words.append(sent2word(i))\n",
        "    return final_words\n",
        "        \n",
        "\n",
        "def noOfWords(essay):\n",
        "    count=0\n",
        "    for i in essay2word(essay):\n",
        "        count=count+len(i)\n",
        "    return count\n",
        "\n",
        "def noOfChar(essay):\n",
        "    count=0\n",
        "    for i in essay2word(essay):\n",
        "        for j in i:\n",
        "            count=count+len(j)\n",
        "    return count\n",
        "def avg_word_len(essay):\n",
        "    return noOfChar(essay)/noOfWords(essay)\n",
        "\n",
        "def noOfSent(essay):\n",
        "    return len(essay2word(essay))\n",
        "\n",
        "def count_pos(essay):\n",
        "    sentences = essay2word(essay)\n",
        "    noun_count=0\n",
        "    adj_count=0\n",
        "    verb_count=0\n",
        "    adverb_count=0\n",
        "    for i in sentences:\n",
        "        pos_sentence = nltk.pos_tag(i)\n",
        "        for j in pos_sentence:\n",
        "            pos_tag = j[1]\n",
        "            if(pos_tag[0]=='N'):\n",
        "                noun_count+=1\n",
        "            elif(pos_tag[0]=='V'):\n",
        "                verb_count+=1\n",
        "            elif(pos_tag[0]=='J'):\n",
        "                adj_count+=1\n",
        "            elif(pos_tag[0]=='R'):\n",
        "                adverb_count+=1\n",
        "    return noun_count,verb_count,adj_count,adverb_count\n",
        "\n",
        "data = open('/content/drive/MyDrive/big').read()\n",
        "words = re.findall('[a-z]+', data.lower())\n",
        "\n",
        "def check_spell_error(essay):\n",
        "    essay=essay.lower()\n",
        "    new_essay = re.sub(\"[^A-Za-z0-9]\",\" \",essay)\n",
        "    new_essay = re.sub(\"[0-9]\",\"\",new_essay)\n",
        "    count=0\n",
        "    all_words = new_essay.split()\n",
        "    for i in all_words:\n",
        "        if i not in words:\n",
        "            count+=1\n",
        "    return count\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-cDNEBoGjs0"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Uu_6VVG-1n"
      },
      "source": [
        "Prep for ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTP7EgMlHH_O"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(max_features = 10000, ngram_range=(0, 3), stop_words='english')\n",
        "count_vectors = vectorizer.fit_transform(df['clean_essay'])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "data = df[['essay_set','clean_essay','final_score']].copy()\n",
        "X = count_vectors.toarray()\n",
        "y = count_vectors.toarray()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByxRIPUjN9pc"
      },
      "outputs": [],
      "source": [
        "#Save Trained Model\n",
        "linear_regressor = LinearRegression()\n",
        "linear_regressor.fit(X_train, y_train)\n",
        "pickle.dump(linear_regressor,open(\"/content/drive/MyDrive/Saved_Models/LR_without_pp\",'wb'))\n",
        "\n",
        "#Use Saved Model\n",
        "model = pickle.load(open(\"/content/drive/MyDrive/Saved_Models/LR_without_pp\",'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LruTcde7_4-"
      },
      "outputs": [],
      "source": [
        "def remove_puncs(essay):\n",
        "    essay = re.sub(\"[^A-Za-z ]\",\"\",essay)\n",
        "    return essay\n",
        "\n",
        "df['clean_essay'] = df['clean_essay'].apply(lambda x:remove_puncs(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxXNMekw8JnG"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9YpfC3-8PFK"
      },
      "outputs": [],
      "source": [
        "def sent2word(x):\n",
        "    x=re.sub(\"[^A-Za-z0-9]\",\" \",x)\n",
        "    words=nltk.word_tokenize(x)\n",
        "    return words\n",
        "\n",
        "def essay2word(essay):\n",
        "    essay = essay.strip()\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw = tokenizer.tokenize(essay)\n",
        "    final_words=[]\n",
        "    for i in raw:\n",
        "        if(len(i)>0):\n",
        "            final_words.append(sent2word(i))\n",
        "    return final_words\n",
        "        \n",
        "\n",
        "def noOfWords(essay):\n",
        "    count=0\n",
        "    for i in essay2word(essay):\n",
        "        count=count+len(i)\n",
        "    return count\n",
        "\n",
        "def noOfChar(essay):\n",
        "    count=0\n",
        "    for i in essay2word(essay):\n",
        "        for j in i:\n",
        "            count=count+len(j)\n",
        "    return count\n",
        "\n",
        "def avg_word_len(essay):\n",
        "    return noOfChar(essay)/noOfWords(essay)\n",
        "\n",
        "def noOfSent(essay):\n",
        "    return len(essay2word(essay))\n",
        "\n",
        "def count_pos(essay):\n",
        "    sentences = essay2word(essay)\n",
        "    noun_count=0\n",
        "    adj_count=0\n",
        "    verb_count=0\n",
        "    adverb_count=0\n",
        "    for i in sentences:\n",
        "        pos_sentence = nltk.pos_tag(i)\n",
        "        for j in pos_sentence:\n",
        "            pos_tag = j[1]\n",
        "            if(pos_tag[0]=='N'):\n",
        "                noun_count+=1\n",
        "            elif(pos_tag[0]=='V'):\n",
        "                verb_count+=1\n",
        "            elif(pos_tag[0]=='J'):\n",
        "                adj_count+=1\n",
        "            elif(pos_tag[0]=='R'):\n",
        "                adverb_count+=1\n",
        "    return noun_count,verb_count,adj_count,adverb_count\n",
        "\n",
        "data = open('/content/drive/MyDrive/big').read()\n",
        "words = re.findall('[a-z]+', data.lower())\n",
        "\n",
        "def check_spell_error(essay):\n",
        "    essay=essay.lower()\n",
        "    new_essay = re.sub(\"[^A-Za-z0-9]\",\" \",essay)\n",
        "    new_essay = re.sub(\"[0-9]\",\"\",new_essay)\n",
        "    count=0\n",
        "    all_words = new_essay.split()\n",
        "    for i in all_words:\n",
        "        if i not in words:\n",
        "            count+=1\n",
        "    return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYeGcjQU9cS2"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwuuskLM9qNL"
      },
      "source": [
        "Prep for ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z24i1on9stO"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(max_features = 10000, ngram_range=(1, 3), stop_words='english')\n",
        "count_vectors = vectorizer.fit_transform(df['clean_essay'])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "data = df[['essay_set','clean_essay','final_score']].copy()\n",
        "X = count_vectors.toarray()\n",
        "y = count_vectors.toarray()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnOcnmJw-SYl"
      },
      "source": [
        "Machine Learning ALGO's without Pre-processing steps\n",
        "\n",
        "Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR-WZlKY-Z8Y"
      },
      "outputs": [],
      "source": [
        "#Save Trained Model\n",
        "linear_regressor = LinearRegression()\n",
        "linear_regressor.fit(X_train, y_train)\n",
        "pickle.dump(linear_regressor,open(\"/content/drive/MyDrive/LR_without_pp\",'wb'))\n",
        "\n",
        "#Use Saved Model\n",
        "model = pickle.load(open(\"/content/drive/MyDrive/LR_without_pp\",'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "104Ct28lYrFhTsU9Fw22d-wNToL3_3YRs",
      "authorship_tag": "ABX9TyMSjPZEzeMhwDQA9RFVjmZw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}